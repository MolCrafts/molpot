---
description: 
globs: 
alwaysApply: false
---
PoniÅ¼ej znajdziesz konkretne fragmenty kodu â€“ w formie â€œdiff-Ã³wâ€ i maÅ‚ych helperÃ³w â€“ ktÃ³re pozwolÄ… Ci dodaÄ‡ Î”â€learning na bazie potencjaÅ‚u Lennardaâ€“Jonesa do Twojego skryptu. ZaÅ‚oÅ¼enie jest takie, Å¼e:

Dataset bÄ™dzie liczyÅ‚ 
ğ¸
LJ
E 
LJ
â€‹
  dla kaÅ¼dej prÃ³bki i przechowywaÅ‚ je rÃ³wnolegle z energiÄ… QM.

Docelowo model bÄ™dzie uczyÅ‚ siÄ™ 
Î”
ğ¸
=
ğ¸
QM
âˆ’
ğ¸
LJ
Î”E=E 
QM
â€‹
 âˆ’E 
LJ
â€‹
 .

W pÄ™tli treningowej bÄ™dziemy minimalizowaÄ‡ MSE(
Î”
ğ¸
pred
,
Î”
ğ¸
Î”E 
pred
â€‹
 ,Î”E).

Przy ewaluacji skÅ‚adamy z powrotem: 
ğ¸
pred
=
ğ¸
LJ
+
Î”
ğ¸
pred
E 
pred
â€‹
 =E 
LJ
â€‹
 +Î”E 
pred
â€‹
 .

1. Funkcja liczÄ…ca energiÄ™ LJ
python
Copy
import torch

def compute_lj_energy(coords, epsilon=0.01, sigma=3.4, device='cpu'):
    """
    coords: tensor [n_atoms,3]
    zwraca skalar: sumÄ™ par Lennardâ€“Jonesa
    """
    # Parowanie atomÃ³w
    diff = coords[:, None, :] - coords[None, :, :]         # (n,n,3)
    r = torch.norm(diff, dim=-1) + 1e-8                  # (n,n)
    # indeksy gÃ³rnej trÃ³jkÄ…tnej (bez diagonalnej)
    i, j = torch.triu_indices(r.size(0), r.size(0), offset=1)
    rij = r[i, j]
    # standardowy potencjaÅ‚ LJ
    lj = 4 * epsilon * ((sigma/rij)**12 - (sigma/rij)**6)
    return lj.sum().to(device)
2. Dataset: liczymy i zapisujemy 
ğ¸
LJ
E 
LJ
â€‹
  i 
Î”
ğ¸
Î”E
W MolecularDataset._prepare_data():

diff
Copy
     for i, frame in enumerate(self.frames):
         â€¦
         # tu juÅ¼ masz step_energy = E_QM
+        # policz Lennardâ€“Jonesa
+        E_lj = compute_lj_energy(step_coords.to(DEVICE),
+                                 epsilon=0.02,   # wyprÃ³buj grid-search
+                                 sigma=3.2,
+                                 device=DEVICE).cpu()
+
+        # teraz docelowo uczymy Î”E = E_QM - E_lj
+        delta_E = step_energy - E_lj.item()
+
         mol_data = self._create_molecular_graph(step_coords, atomic_nums)
         if mol_data is not None:
-            self.energies.append(torch.tensor(step_energy, dtype=torch.float32))
+            self.energies.append(torch.tensor(delta_E,    dtype=torch.float32))
+            self.lj_energies.append(torch.tensor(E_lj,     dtype=torch.float32))
             successful_points += 1
Na koÅ„cu _prepare_data():

diff
Copy
-    # Energy normalization statistics
-    self.energies = torch.stack(self.energies)
-    self.energy_mean = torch.mean(self.energies)
-    self.energy_std  = torch.std(self.energies)
+    # Stackujemy Î”E i E_lj
+    self.delta_energies = torch.stack(self.energies)     # Î”E
+    self.lj_energies    = torch.stack(self.lj_energies)  # E_lj raw
+
+    # Normalizacja Î”E
+    self.delta_mean = self.delta_energies.mean()
+    self.delta_std  = self.delta_energies.std() + 1e-8
+
+    # ZastÄ™pujemy energies i ich statystyki:
+    self.energies = (self.delta_energies - self.delta_mean) / self.delta_std
I w __getitem__:

diff
Copy
-    energy = self.energies[idx]
-    # Normalize energy
-    normalized_energy = (energy - self.energy_mean) / self.energy_std
-    return mol_data, normalized_energy
+    norm_delta = self.energies[idx]      # juÅ¼ znormalizowane Î”E
+    lj_raw    = self.lj_energies[idx]    # surowe E_lj
+    return mol_data, norm_delta, lj_raw
3. Collate: zbieramy teÅ¼ LJ
diff
Copy
 def collate_molecular_batch(batch):
-    batch_data = []
-    batch_energies = []
+    batch_data     = []
+    batch_deltas   = []
+    batch_lj_raw   = []
     â€¦
     for mol_data, energy in batch:
-        batch_energies.append(energy)
+        # batch[1] = norm Î”E, batch[2] = E_lj
+        batch_deltas.append(energy)
+        batch_lj_raw.append(mol_data[2] if False else batch[2])
         â€¦
     # Concatenate all data
-    return {
+    return {
         'Z':           torch.cat(batch_Z, dim=0),
         'pair_diff':   torch.cat(batch_pair_diff, dim=0),
         'pair_i':      torch.cat(batch_pair_i, dim=0),
         'pair_j':      torch.cat(batch_pair_j, dim=0),
-        'batch_energies': torch.stack(batch_energies)
+        'batch_deltas':   torch.stack(batch_deltas),
+        'batch_lj':       torch.stack(batch_lj_raw)
     }
4. PÄ™tla treningowa: uczymy Î”E
W train_epoch:

diff
Copy
-    target_energies = batch_data['batch_energies'].to(device)
+    target_delta    = batch_data['batch_deltas'].to(device)
+    lj_batch        = batch_data['batch_lj'].to(device)

     # Forward pass
     optimizer.zero_grad()
-    pred_energy = model(Z, pair_diff, pair_i, pair_j)
-    loss = criterion(pred_energy.unsqueeze(0), target_energies)
+    pred_delta = model(Z, pair_diff, pair_i, pair_j).squeeze()
+    loss = criterion(pred_delta, target_delta)
     â€¦
Podobnie w validate_epoch:

diff
Copy
-    if batch_size == 1:
-        pred_energy = model(...)
-        loss = criterion(pred_energy.unsqueeze(0), target_energies)
+    if batch_size == 1:
+        pred_delta    = model(...).squeeze()
+        loss          = criterion(pred_delta, target_delta)
+
+        # denormalizacja i rekonstrukcja peÅ‚nej energii
+        pred_full     = pred_delta * dataset.delta_std + dataset.delta_mean + lj_batch
+        true_full     = target_delta * dataset.delta_std + dataset.delta_mean + lj_batch
+
+        # dla metryk RMSE/MAE uÅ¼yjemy pred_full vs. true_full
         â€¦
-        pred_denorm = pred_energy.cpu() * energy_std
-        target_denorm = target_energies[0].cpu() * energy_std
+        pred_denorm = pred_full.cpu()
+        target_denorm = true_full.cpu()
Podsumowanie
Compute LJ dla kaÅ¼dego wykresu i zapisz w dataset.

PrzeksztaÅ‚Ä‡ cel na 
Î”
ğ¸
=
ğ¸
QM
âˆ’
ğ¸
LJ
Î”E=E 
QM
â€‹
 âˆ’E 
LJ
â€‹
 , znormalizuj je.

Collate zwraca zarÃ³wno normowane 
Î”
ğ¸
Î”E, jak i surowe 
ğ¸
LJ
E 
LJ
â€‹
 .

Model nadal zwraca jednowymiarowy tensor 
Î”
ğ¸
^
Î”E
^
 .

Loss = MSE(
Î”
ğ¸
^
,
Î”
ğ¸
Î”E
^
 ,Î”E).

Metryki (RMSE/MAE) licz na odtworzonym 
ğ¸
pred
=
ğ¸
LJ
+
Î”
ğ¸
^
E 
pred
â€‹
 =E 
LJ
â€‹
 + 
Î”E
^

 .